{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime as dt\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input your `client_id` , `client_secret` , `user_agent`, `username`, `password`\n",
    "\n",
    "#### Note: sensitive information has been removed from this repo for privacy reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=\"<your_client_id>\",#my client id\n",
    "                     client_secret=\"<your_client_secret>\",  #your client secret\n",
    "                     user_agent=\"<your_user_agent>\", #user agent name\n",
    "                     username = \"<your_username>\",     # your reddit username\n",
    "                     password = \"<your_password>\")     # your reddit password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to scrape new submissions and comments from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit for scraping code to https://github.com/parth647/reddit_scraping_using_praw/blob/master/python_reddit_scrapy.py\n",
    "\n",
    "#   NOTE: ALL THE POST DATA AND COMMENT DATA WILL BE SAVED IN TWO DIFFERENT\n",
    "#   DATASETS AND LATER CAN BE MAPPED USING IDS OF POSTS/COMMENTS AS WE WILL \n",
    "#   BE CAPTURING ALL IDS THAT COME IN OUR WAY\n",
    "\n",
    "def scrape_reddit(subreddit_name, lim):\n",
    "    \n",
    "    #timer\n",
    "    t0 = time.time()\n",
    "    \n",
    "    sub = [subreddit_name]  # make a list of subreddits you want to scrape the data from\n",
    "\n",
    "    # Accessing the reddit api\n",
    "    for s in sub:\n",
    "        subreddit = reddit.subreddit(s)   # Chosing the subreddit\n",
    "\n",
    "        #   CREATING DICTIONARY TO STORE THE DATA WHICH WILL BE CONVERTED TO A DATAFRAME\n",
    "        post_dict = {\n",
    "            \"subreddit\" : [],\n",
    "            \"id\" : [],\n",
    "            \"comment_link_id\" : [],\n",
    "            \"title\" : [],\n",
    "            \"selftext\" : [],\n",
    "            \"score\" : [],\n",
    "            \"upvote_ratio\" : [],\n",
    "            \"num_comments\" : [],\n",
    "            \"flair\" : [],\n",
    "            \"url\" : [],\n",
    "            \"author\": [],\n",
    "            \"created\" : []\n",
    "        }\n",
    "        \n",
    "        comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_link_id\" : []\n",
    "        }\n",
    "\n",
    "    # SCRAPING CAN BE DONE VIA VARIOUS STRATEGIES {HOT,TOP,etc}\n",
    "    \n",
    "        #pulls new submissions\n",
    "        for submission in subreddit.new(limit=lim):         \n",
    "            post_dict[\"subreddit\"].append(submission.subreddit)\n",
    "            post_dict[\"id\"].append(submission.id)\n",
    "            post_dict[\"title\"].append(submission.title)\n",
    "            post_dict[\"score\"].append(submission.score)\n",
    "            post_dict[\"url\"].append(submission.url)\n",
    "            post_dict[\"author\"].append(submission.author)\n",
    "            post_dict[\"created\"].append(submission.created_utc)\n",
    "            post_dict[\"selftext\"].append(submission.selftext)\n",
    "            post_dict[\"comment_link_id\"].append(submission.name)\n",
    "            post_dict[\"flair\"].append(submission.link_flair_text)\n",
    "            post_dict[\"upvote_ratio\"].append(submission.upvote_ratio)\n",
    "            post_dict[\"num_comments\"].append(submission.num_comments)\n",
    "\n",
    "            ##### Acessing comments on the post\n",
    "            submission.comments.replace_more(limit = 1)\n",
    "            for comment in submission.comments.list():\n",
    "                comments_dict[\"comment_id\"].append(comment.id)\n",
    "                comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "                comments_dict[\"comment_body\"].append(comment.body)\n",
    "                comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
    "        \n",
    "        #create the dataframes\n",
    "        post_comments = pd.DataFrame(comments_dict)\n",
    "        post_data = pd.DataFrame(post_dict)\n",
    "        \n",
    "        #timer\n",
    "        print(f'Scrape Complete. Duration: {(time.time() - t0) / 60} mins')\n",
    "\n",
    "        return post_comments.to_csv(\"./data/\" + s + \"_comments_subreddit.csv\"), post_data.to_csv(\"./data/\" + s + \"_subreddit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape Complete. Duration: 15.226985983053844 mins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alexa\n",
    "scrape_reddit('alexa', lim=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape Complete. Duration: 16.239685984452567 mins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#googlehome\n",
    "scrape_reddit('googlehome', lim=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
